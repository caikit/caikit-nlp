{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpDqGEKmVb0IdpyD19mhFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rawkintrevo/caikit-nlp/blob/101-b/examples/Caikit_Getting_Started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Caikit\n",
        "\n",
        "## Installation and Setup\n",
        "\n",
        "In this example Jupyter notebook, we'll be using various Python libraries and pre-trained models for evaluating and analyzing natural language processing tasks. Before we proceed, we need to install the required dependencies and download some essential resources.\n",
        "\n",
        "### 1. Installing Libraries\n",
        "\n",
        "To begin, we'll install the following Python packages using `pip`:\n",
        "\n",
        "- `evaluate`: A library for evaluating model performance on different NLP tasks.\n",
        "- `rouge_score`: A package for calculating ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics for text summarization.\n",
        "\n",
        "Please note that these libraries may have dependencies, so we'll ensure all the necessary requirements are met during the installation process.\n",
        "\n",
        "```python\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "```\n",
        "\n",
        "### 2. Installing `caikit` and `caikit-nlp`\n",
        "\n",
        "Next, we'll install specific versions of the caikit and caikit-nlp libraries, as the project is still in beta and breaking changes can happen.\n",
        "\n",
        "```python\n",
        "!pip install git+https://github.com/caikit/caikit@v0.11.3\n",
        "!pip install git+https://github.com/caikit/caikit-nlp\n",
        "```\n",
        "\n",
        "### 3. Downloading Additional Resources\n",
        "\n",
        "In order to explore the capabilities of pre-trained models, we'll need to download two clone the repo:\n",
        "\n",
        "* The caikit-nlp repository: We'll clone this repository to access some additional NLP utilities that might come in handy during our analysis.\n",
        "\n",
        "```python\n",
        "!git clone https://github.com/caikit/caikit-nlp\n",
        "```\n",
        "\n",
        "Now that we have all the necessary libraries and resources installed, we can move on to the next steps in our NLP analysis using these powerful tools!"
      ],
      "metadata": {
        "id": "nqAU3Yh-rha5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhZcVULDrTRz"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "\n",
        "!pip install git+https://github.com/caikit/caikit@v0.11.3\n",
        "!pip install git+https://github.com/caikit/caikit-nlp\n",
        "\n",
        "!git clone https://github.com/caikit/caikit-nlp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Prompt Tuning\n",
        "\n",
        "```\n",
        "!python caikit-nlp/examples/run_peft_tuning.py MULTITASK_PROMPT_TUNING \\\n",
        "      --dataset \"glue/rte\"  \\\n",
        "      --model_name t5-base \\\n",
        "      --num_epochs 1 \\\n",
        "      --verbose \\\n",
        "      --num_virtual_tokens 100 \\\n",
        "      --prompt_tuning_init RANDOM  \\\n",
        "      --output_dir tmp/foo \\\n",
        "      --learning_rate 0.3 \\\n",
        "      --batch_size=12 \\\n",
        "      --accumulate_steps 16\n",
        "```\n",
        "\n",
        "This is a command-line instruction to run a Python script called `run_peft_tuning.py` using the python interpreter. It is part of the `caikit-nlp` package and is meant to tune a pre-trained model (in this case t5-base) on a specific dataset (in this case glue/rte) using different parameter efficient tuning approaches, e.g., MPT.\n",
        "\n",
        "Let's explain each argument in the command:\n",
        "\n",
        "1. `caikit-nlp/examples/run_peft_tuning.py`: This specifies the path to the Python script that will be executed. It is a part of the caikit-nlp library and contains the implementation of the PEFT approach.\n",
        "1. `MULTITASK_PROMPT_TUNING`: This is the option recognized by the `run_peft_tuning.py` script, indicating the type of tuning or learning strategy to be used. In this case, it's \"Multitask Prompt Tuning.\" (The alternative option is `PROMPT_TUNING` which has slightly different options. )\n",
        "1. `--dataset \"glue/rte\"`: This specifies the dataset to be used for tuning. In this example, the dataset is `glue/rte` which is subset of the GLUE (General Language Understanding Evaluation) benchmark containing the Recognizing Textual Entailment (RTE) task.\n",
        "1. `--model_name t5-base`: This indicates the base model that will be used for prompt tuning. In this case, it's `t5-base`, which refers to the T5-base model from Hugging Face. **NOTE: At the time of writing the model must exist locally, this is why in the last step we cloned**`t5-base` ** to ** `./t5-base`\n",
        "1. `--num_epochs 1`: This sets the number of epochs (training iterations) for the prompt-tuning process. Here, it's set to 1, meaning the model will go through the dataset once during prompt-tuning.\n",
        "1. ` --verbose`: This is a flag to enable verbose or detailed output during the training process. It will cause the script to print more information about the training progress.\n",
        "1. `--num_virtual_tokens 100`: This sets the number of virtual tokens used for prompt tuning. Prompt tuning involves freezing the weights of a base model and learning `soft prompts`, which can be concatenated to the inputs when running text generation on the model.\n",
        "1. `--prompt_tuning_init RANDOM`: This specifies the method used for initializing prompt tuning. In this case, it's set to \"RANDOM,\" meaning the prompt tuning will start with random values.\n",
        "1. `--output_dir tmp/foo`: This sets the directory where the prompt-tuned model and related outputs will be stored. In this case, it's set to the `tmp/foo` directory.\n",
        "1. `--learning_rate 0.3`: This sets the learning rate for the optimization process during prompt-tuning. The learning rate controls the step size in gradient descent.\n",
        "1. `--batch_size=12`: This sets the batch size used during training. The data will be divided into batches of 12 samples each.\n",
        "1. `--accumulate_steps 16`: This specifies the number of steps before gradients are accumulated and the weights are updated. It can be useful for larger batch sizes when the GPU memory is limited.\n",
        "\n",
        "Overall, this command line script is prompt-tuning the T5-base model using the MPT approach on the `glue/rte` dataset, with specific settings for learning rate, batch size, accumulation steps, and so on. For a full list of available args, their descriptions, and default values, run `!python caikit-nlp/examples/run_peft_tuning.py MULTITASK_PROMPT_TUNING --help`
        
        The results of prompt-tuning will be stored in the `tmp/foo` directory."
      ],
      "metadata": {
        "id": "7RIXUii94aLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env ALLOW_DOWNLOADS=true"
        "!python caikit-nlp/examples/run_peft_tuning.py MULTITASK_PROMPT_TUNING --dataset \"glue/rte\"  --model_name t5-base --num_epochs 1 --verbose --num_virtual_tokens 100 --prompt_tuning_init RANDOM  --output_dir tmp/foo --learning_rate 0.3 --batch_size=12 --accumulate_steps 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4deByI5aRNQ",
        "outputId": "3495de55-6a2f-4517-9165-8560b71442ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-26 15:18:15.365409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "<function register_backend_type at 0x789628fda4d0> is still in the BETA phase and subject to change!\n",
            "Could not deduct output type from function train for module class FineTuning.\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 14.1MB/s]\n",
            "Downloading builder script: 100% 6.60k/6.60k [00:00<00:00, 27.3MB/s]\n",
            "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 28.6MB/s]\n",
            "\u001b[94mExperiment Configuration\n",
            "- Model Name: [t5-base]\n",
            " |- Inferred Model Resource Type: [<class 'caikit_nlp.resources.pretrained_model.hf_auto_seq2seq_lm.HFAutoSeq2SeqLM'>]\n",
            "- Tuning Type: [MULTITASK_PROMPT_TUNING]\n",
            "- Prompt Tuning Initialization Type [RANDOM]\n",
            "- Number of Virtual Tokens: [100]\n",
            "- Dataset: [glue/rte]\n",
            "- Verbalizer: [rte { 0 : entailment, 1 : not entailment } {{input}}]\n",
            "- Number of Epochs: [1]\n",
            "- Learning Rate: [0.3]\n",
            "- Batch Size: [12]\n",
            "- Output Directory: [tmp/foo]\n",
            "- Exporting prompt only: [False]\n",
            "- Number of shots: [None]\n",
            "- Maximum source sequence length: [256]\n",
            "- Maximum target sequence length: [128]\n",
            "- Gradient accumulation steps: [16]\u001b[0m\n",
            "\u001b[94m[Loading the dataset...]\u001b[0m\n",
            "Downloading builder script: 100% 28.8k/28.8k [00:00<00:00, 56.6MB/s]\n",
            "Downloading metadata: 100% 28.7k/28.7k [00:00<00:00, 51.5MB/s]\n",
            "Downloading readme: 100% 27.9k/27.9k [00:00<00:00, 42.3MB/s]\n",
            "Downloading data: 100% 697k/697k [00:00<00:00, 21.2MB/s]\n",
            "Generating train split:   0% 0/2490 [00:00<?, ? examples/s]2023-07-26T15:18:25.574714 [fsspe:DBUG] open file: /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad.incomplete/glue-train-00000-00000-of-NNNNN.arrow\n",
            "Generating train split: 100% 2490/2490 [00:00<00:00, 7160.61 examples/s]\n",
            "Generating validation split:   0% 0/277 [00:00<?, ? examples/s]2023-07-26T15:18:25.923005 [fsspe:DBUG] open file: /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad.incomplete/glue-validation-00000-00000-of-NNNNN.arrow\n",
            "Generating validation split: 100% 277/277 [00:00<00:00, 10849.23 examples/s]\n",
            "Generating test split:   0% 0/3000 [00:00<?, ? examples/s]2023-07-26T15:18:25.948941 [fsspe:DBUG] open file: /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad.incomplete/glue-test-00000-00000-of-NNNNN.arrow\n",
            "Generating test split: 100% 3000/3000 [00:00<00:00, 18902.94 examples/s]\n",
            "2023-07-26T15:18:26.107969 [fsspe:DBUG] open file: /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad.incomplete/dataset_info.json\n",
            "\u001b[94m[Loading the base model resource...]\u001b[0m\n",
            "\u001b[94m[Starting the training...]\u001b[0m\n",
            "2023-07-26T15:18:33.524743 [PEFT_:INFO] Using initialization method [MultitaskPromptTuningInit.RANDOM]\n",
            "2023-07-26T15:18:34.929722 [PEFT_:DBUG] Shuffling enabled? True\n",
            "2023-07-26T15:18:34.929855 [PEFT_:DBUG] Shuffling buffer size: 2490\n",
            "2023-07-26T15:18:34.930223 [PEFT_:INFO] <NLP18184771I> [{'prompt_tuning_init_source_model', 'prompt_tuning_init_method', 'output_model_types'}] config params not supported by provided tuning type!\n",
            "2023-07-26T15:18:34.930334 [PEFT_:INFO] <NLP41038481I> Parameters used: {'num_virtual_tokens': 100, 'prompt_tuning_init_text': None, 'prompt_tuning_init_state_dict_path': None, 'tokenizer_name_or_path': 't5-base', 'num_transformer_submodules': 1, 'prompt_tuning_init': 'RANDOM'}\n",
            "2023-07-26T15:18:34.930405 [PEFT_:DBUG] Peft config [MultitaskPromptTuningConfig(peft_type=<PeftType.MULTITASK_PROMPT_TUNING: 'MULTITASK_PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, inference_mode=False, num_virtual_tokens=100, token_dim=None, num_transformer_submodules=1, num_attention_heads=None, num_layers=None, prompt_tuning_init='RANDOM', prompt_tuning_init_text=None, tokenizer_name_or_path='t5-base', prompt_tuning_init_state_dict_path=None, prompt_tuning_init_task=0, num_ranks=1, num_tasks=1)]\n",
            "2023-07-26T15:18:34.947495 [PEFT_:DBUG] Using device: cuda\n",
            "Epoch: 0: 100% 208/208 [03:12<00:00,  1.08it/s]\n",
            "2023-07-26T15:21:52.737027 [PEFT_:INFO] epoch 0: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "2023-07-26T15:21:52.918779 [PEFT_:INFO] Calculating single target task prompt vector\n",
            "output_type:  PromptOutputModelType.ENCODER\n",
            "\u001b[94m[Training Complete]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6jfh5nSwP48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
